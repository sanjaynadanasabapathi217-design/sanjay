# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1759fqTBlD5iFUZERzhqVXOYUPTq3RxUZ
"""

"""
Advanced Time Series Forecasting with Attention-Based Neural Networks
Description:
- Generates a complex multivariate time series dataset
- Implements a baseline LSTM model
- Implements an Attention-based Transformer model
- Compares performance using RMSE, MAE, MAPE
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.preprocessing import MinMaxScaler
import math
import random

# =============================
# 1. Reproducibility
# =============================
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
random.seed(SEED)

# =============================
# 2. Data Generation
# =============================
def generate_multivariate_time_series(n_steps=1500):
    """
    Generates a multivariate time series with trend, seasonality, noise,
    and correlated exogenous variables.
    """
    t = np.arange(n_steps)

    trend = 0.01 * t
    seasonality = np.sin(2 * np.pi * t / 50)
    noise = np.random.normal(0, 0.3, n_steps)

    feature_1 = trend + seasonality + noise
    feature_2 = np.cos(2 * np.pi * t / 30) + noise * 0.5
    feature_3 = 0.5 * trend + np.random.normal(0, 0.2, n_steps)
    feature_4 = np.sin(2 * np.pi * t / 100)
    feature_5 = feature_1 * 0.3 + feature_2 * 0.7

    data = np.vstack([
        feature_1,
        feature_2,
        feature_3,
        feature_4,
        feature_5
    ]).T

    return pd.DataFrame(
        data,
        columns=[f"feature_{i}" for i in range(1, 6)]
    )

# =============================
# 3. Dataset Preparation
# =============================
class TimeSeriesDataset(Dataset):
    def __init__(self, data, seq_len=30):
        self.data = data
        self.seq_len = seq_len

    def __len__(self):
        return len(self.data) - self.seq_len

    def __getitem__(self, idx):
        x = self.data[idx:idx + self.seq_len]
        y = self.data[idx + self.seq_len, 0]  # predict main variable
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

# =============================
# 4. Baseline LSTM Model
# =============================
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size=64):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :]).squeeze()

# =============================
# 5. Transformer Attention Model
# =============================
class TransformerModel(nn.Module):
    def __init__(self, input_size, d_model=64, nhead=4, num_layers=2):
        super().__init__()

        self.embedding = nn.Linear(input_size, d_model)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        self.fc = nn.Linear(d_model, 1)

    def forward(self, x):
        x = self.embedding(x)
        x = self.transformer(x)
        return self.fc(x[:, -1, :]).squeeze()

# =============================
# 6. Training & Evaluation
# =============================
def train_model(model, loader, optimizer, criterion, epochs=20):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for x, y in loader:
            optimizer.zero_grad()
            preds = model(x)
            loss = criterion(preds, y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1}/{epochs} - Loss: {total_loss / len(loader):.4f}")

def evaluate_model(model, loader):
    model.eval()
    y_true, y_pred = [], []

    with torch.no_grad():
        for x, y in loader:
            preds = model(x)
            y_true.extend(y.numpy())
            y_pred.extend(preds.numpy())

    rmse = math.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    mape = np.mean(np.abs((np.array(y_true) - np.array(y_pred)) / np.array(y_true))) * 100

    return rmse, mae, mape

# =============================
# 7. Main Execution
# =============================
if __name__ == "__main__":

    # Generate Data
    df = generate_multivariate_time_series()
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(df.values)

    # Split Data (70/15/15)
    train_size = int(0.7 * len(scaled_data))
    val_size = int(0.85 * len(scaled_data))

    train_data = scaled_data[:train_size]
    val_data = scaled_data[train_size:val_size]
    test_data = scaled_data[val_size:]

    seq_len = 30
    batch_size = 32

    train_loader = DataLoader(TimeSeriesDataset(train_data, seq_len), batch_size=batch_size, shuffle=False)
    val_loader = DataLoader(TimeSeriesDataset(val_data, seq_len), batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(TimeSeriesDataset(test_data, seq_len), batch_size=batch_size, shuffle=False)

    # =============================
    # Train LSTM Baseline
    # =============================
    print("\nTraining LSTM Baseline Model")
    lstm_model = LSTMModel(input_size=5)
    optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)
    criterion = nn.MSELoss()

    train_model(lstm_model, train_loader, optimizer, criterion)
    lstm_metrics = evaluate_model(lstm_model, test_loader)

    # =============================
    # Train Transformer Model
    # =============================
    print("\nTraining Attention-Based Transformer Model")
    transformer_model = TransformerModel(input_size=5)
    optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)

    train_model(transformer_model, train_loader, optimizer, criterion)
    transformer_metrics = evaluate_model(transformer_model, test_loader)

    # =============================
    # Results
    # =============================
    print("\nModel Comparison (Test Set)")
    print("--------------------------------")
    print(f"LSTM      -> RMSE: {lstm_metrics[0]:.4f}, MAE: {lstm_metrics[1]:.4f}, MAPE: {lstm_metrics[2]:.2f}%")
    print(f"Transformer -> RMSE: {transformer_metrics[0]:.4f}, MAE: {transformer_metrics[1]:.4f}, MAPE: {transformer_metrics[2]:.2f}%")